{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7008f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\hp\\miniconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: openai in c:\\users\\hp\\miniconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\hp\\miniconda3\\lib\\site-packages (0.11.0)\n",
      "Collecting rapidocr-onnxruntime\n",
      "  Downloading rapidocr_onnxruntime-1.2.3-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain) (0.3.78)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain) (0.4.32)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tiktoken) (2025.9.18)\n",
      "Collecting pyclipper>=1.2.1 (from rapidocr-onnxruntime)\n",
      "  Downloading pyclipper-1.3.0.post6-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: onnxruntime>=1.7.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from rapidocr-onnxruntime) (1.23.0)\n",
      "Collecting opencv-python>=4.5.1.48 (from rapidocr-onnxruntime)\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from rapidocr-onnxruntime) (2.3.3)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from rapidocr-onnxruntime) (1.17.0)\n",
      "Collecting Shapely>=1.7.1 (from rapidocr-onnxruntime)\n",
      "  Downloading shapely-2.1.2-cp313-cp313-win_amd64.whl.metadata (7.1 kB)\n",
      "Collecting Pillow (from rapidocr-onnxruntime)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\miniconda3\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\miniconda3\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (25.9.23)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\miniconda3\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (6.32.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\miniconda3\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.14.0)\n",
      "Collecting numpy>=1.19.3 (from rapidocr-onnxruntime)\n",
      "  Downloading numpy-2.2.6-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime) (3.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
      "Downloading rapidocr_onnxruntime-1.2.3-py3-none-any.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.1/12.3 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.3 MB 14.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.3 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.3 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 12.9 MB/s eta 0:00:00\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "   ---------------------------------------- 0.0/39.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.7/39.0 MB 18.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 6.3/39.0 MB 15.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 7.9/39.0 MB 14.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 11.8/39.0 MB 14.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 14.7/39.0 MB 13.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 17.3/39.0 MB 13.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.9/39.0 MB 13.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.8/39.0 MB 13.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.7/39.0 MB 13.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.3/39.0 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.9/39.0 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.8/39.0 MB 13.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.4/39.0 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.0 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.0/39.0 MB 12.8 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.6-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.9/12.6 MB 19.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.6/12.6 MB 16.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 15.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading pyclipper-1.3.0.post6-cp313-cp313-win_amd64.whl (109 kB)\n",
      "Downloading shapely-2.1.2-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 17.7 MB/s eta 0:00:00\n",
      "Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 3.7/7.0 MB 18.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 13.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pyclipper, Pillow, numpy, Shapely, opencv-python, rapidocr-onnxruntime\n",
      "\n",
      "   ------ --------------------------------- 1/6 [Pillow]\n",
      "   ------ --------------------------------- 1/6 [Pillow]\n",
      "   ------ --------------------------------- 1/6 [Pillow]\n",
      "   ------ --------------------------------- 1/6 [Pillow]\n",
      "   ------ --------------------------------- 1/6 [Pillow]\n",
      "   ------ --------------------------------- 1/6 [Pillow]\n",
      "  Attempting uninstall: numpy\n",
      "   ------ --------------------------------- 1/6 [Pillow]\n",
      "    Found existing installation: numpy 2.3.3\n",
      "   ------ --------------------------------- 1/6 [Pillow]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "    Uninstalling numpy-2.3.3:\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "      Successfully uninstalled numpy-2.3.3\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   -------------------- ------------------- 3/6 [Shapely]\n",
      "   -------------------- ------------------- 3/6 [Shapely]\n",
      "   -------------------- ------------------- 3/6 [Shapely]\n",
      "   -------------------- ------------------- 3/6 [Shapely]\n",
      "   -------------------------- ------------- 4/6 [opencv-python]\n",
      "   -------------------------- ------------- 4/6 [opencv-python]\n",
      "   -------------------------- ------------- 4/6 [opencv-python]\n",
      "   -------------------------- ------------- 4/6 [opencv-python]\n",
      "   --------------------------------- ------ 5/6 [rapidocr-onnxruntime]\n",
      "   ---------------------------------------- 6/6 [rapidocr-onnxruntime]\n",
      "\n",
      "Successfully installed Pillow-11.3.0 Shapely-2.1.2 numpy-2.2.6 opencv-python-4.12.0.88 pyclipper-1.3.0.post6 rapidocr-onnxruntime-1.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai tiktoken rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c7b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f9aa2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01c284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "addc3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data.txt\",\"r\", encoding=\"utf8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fab442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loder=TextLoader('train_data.txt' , encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872b08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=loder.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f463e11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you manage information across modalities?\n",
      "Another crucial aspect is representing information across different modalities. For instance, if you are working with a document, you must make sure that the semantic representation of a chart aligns with the semantic representation of the text discussing the same chart.\n",
      "\n",
      "Approaches for multimodal retrieval\n",
      "With the key challenge understood, here are the specifics of building RAG pipelines to tackle these challenges.\n",
      "\n",
      "There are several main approaches to building multi-modal RAG pipelines:\n",
      "\n",
      "Embed all modalities into the same vector space\n",
      "Ground all modalities into one primary modality\n",
      "Have separate stores for different modalities\n",
      "To keep this discussion concise, we only discuss images and text input.\n",
      "\n",
      "Embed all modalities into the same vector space\n",
      "In the case of images and text, you can use a model like CLIP to encode both text and images in the same vector space. This makes it so that you can largely use the same text-only RAG infrastructure and swap out the embedding model to accommodate another modality. For the generation pass, you then replace the large language model (LLM) with a multimodal LLM (MLLM) for all question and answering.\n",
      "\n",
      "This approach simplifies the pipeline, as the only change required in the generic retrieval pipeline is that of swapping the embedding model. \n",
      "\n",
      "The tradeoff in this situation is to have access to a model that can effectively embed different types of images and text and also capture all the intricacies like text in images and complex tables.\n",
      "\n",
      "Ground all modalities into one primary modality\n",
      "Another option is to pick a primary modality based on the focus of the application and ground all other modalities in the primary modality. \n",
      "\n",
      "For example, say your application revolves mainly around text-based Q&A over PDFs. In this case, you process text normally but for images, you create text descriptions and metadata in the preprocessing step. You also store the images for later use. \n",
      "\n",
      "In the inference pass, the retrieval then works primarily off of the text description and metadata for the images, and the answer is generated with a mix of LLMs and MLLMs, depending on the type of image retrieved. \n",
      "\n",
      "The key benefit here is that the metadata generated from the information-rich image is extremely helpful in answering objective questions. This also works around the need for tuning a new model for embedding images as well as building a re-ranker to rank results from across different modalities. The key disadvantages are preprocessing costs and losing some nuance from the image. \n",
      "\n",
      "Have separate stores for different modalities\n",
      "Rank-rerank is another approach where you have separate stores for different modalities, query them all to retrieve top-N chunks, and then have a dedicated multimodal re-ranker provide the most relevant chunks. \n",
      "\n",
      "This approach simplifies the modeling process, so that you don’t have to align one model to work with multiple modalities. However, it adds complexity in the form of a re-ranker to arrange the now top-M*N chunks (N each from M modalities). \n",
      "\n",
      "Multimodal models for generation\n",
      "LLMs are designed to understand, interpret, and generate text-based information. Trained on vast amounts of textual data, LLMs can perform a range of natural language processing tasks, such as text generation, summarization, question-answering, and more.\n",
      "\n",
      "MLLMs can perceive more than textual data. MLLMs can handle modalities like images, audio, and video, which is often how real-world data is composed. They combine these different data types to create a more comprehensive interpretation of the information, improving the accuracy and robustness of its predictions. \n",
      "\n",
      "These models can perform a wide range of tasks:\n",
      "\n",
      "Visual language understanding and generation\n",
      "Multimodal dialogue\n",
      "Image captioning \n",
      "Visual question answering (VQA) \n",
      "These are all tasks that a RAG system can benefit from while dealing with multiple modalities. Having a deeper understanding of how MLLMs work with images and text requires a look at how these models are constructed. \n",
      "\n",
      "One of the popular subtypes of MLLMs is Pix2Struct, a pretrained image-to-text model that enables semantic understanding of the visual input with its novel pretraining strategy. These models, as the name suggests, generate structured information extracted from the image. For instance, a Pix2Struct model can extract the key information from charts and express it in text.\n",
      "\n",
      "With that understood, here’s how you can build a RAG pipeline.\n",
      "\n",
      "Building a pipeline for multimodal RAG\n",
      "To showcase how you can tackle different modalities of data, we walk you through an application indexing multiple technical posts, such as Breaking MLPerf Training Records with NVIDIA H100 GPUs. This post contains complex images that are charts and graphs with rich text, tabular data, and of course, paragraphs. \n",
      "\n",
      "Here are the models and tools that you need before you can begin handling the data and building a RAG pipeline:\n",
      "\n",
      "MLLM: Used for image captioning and VQA.\n",
      "LLM: General reasoning and question answering.\n",
      "Embedding model: Encoding data into vectors.\n",
      "Vector database: Store the encoded vectors for retrieval.\n",
      "Interpreting multimodal data and creating a vector database\n",
      "The first step for building a RAG application is to preprocess your data and store it as vectors in a vector store so that you can then retrieve relevant vectors based on a query. \n",
      "\n",
      "With images present in the data, here is a generic RAG preprocessing workflow to work through (Figure 2). \n",
      "\n",
      "Demonstration of RAG preprocessing workflow for handling data containing images, showcasing the sequence of steps from data ingestion to processing and analysis.\n",
      "Figure 2. General RAG preprocessing workflow for data with images\n",
      "The post in question contains several bar charts like Figure 3. To interpret these bar charts, use Google’s DePlot, a visual-language model capable of comprehending charts and plots when coupled with an LLM. The model is available on NGC.\n",
      "\n",
      "For more information about using the DePlot API in your RAG application, see Query Graphs with Optimized DePlot Model.\n",
      "\n",
      "A bar chart image to be analyzed with an MLLM\n",
      "Figure 3. Example bar chart from the PDF, including the caption\n",
      "This example focuses on charts and plots. Other documents may contain images that may require model customization to handle specialized images, such as medical imagery or schematic diagrams. This depends on the use case but you have several options to tackle this variance in imagery: Either tune one MLLM to handle all types of imagery or to build an ensemble of models for different types of images. \n",
      "\n",
      "To keep the explanation simple, this is a simple ensemble case with two categories: \n",
      "\n",
      "Image with graphs to be processed using DePlot\n",
      "Other images to be processed with an MLLM like KOSMOS2\n",
      "In this post, we expand the preprocessing pipeline to dive deeper into handling each modality in the pipeline that leverages custom text splitters, customized MLLMs, and LLMs to create the VectorDB (Figure 4).\n",
      "\n",
      "Diagram shows extracting text, separating images and text, building image descriptions, linearizing tables as text, splitting text, and other processing steps.\n",
      "Figure 4. RAG preprocessing workflow with customized MLLMs tackling different types of images\n",
      "Here are some key steps in the preprocessing workflow:\n",
      "\n",
      "Separate images and text\n",
      "Classify images using an MLLM based on the image types\n",
      "Embed text in PDFs\n",
      "Separate images and text\n",
      "The goal is to ground images to text modality. Start by extracting and cleaning your data to separate images and text. You can then go about tackling these two modalities to eventually store them in the vector store.\n",
      "\n",
      "Classify images using an MLLM based on the image types\n",
      "Image descriptions generated by an MLLM can be used to classify images into categories whether or not they are graphs. Based on the classification, use DePlot for images containing graphs to generate a linearized tabular text. This text being semantically different from regular text poses a challenge for retrieving the relevant information when performing a search during inference.\n",
      "\n",
      "We recommend using summaries of the linearized text as chunks to store in the vector store with the outputs from customized MLLMs as metadata, which you can use during inference.\n",
      "\n",
      "Embed text in PDFs\n",
      "There is room for exploring various text-splitting techniques based on the data that you are working with to achieve the best RAG performance. For simplicity, store each paragraph as a chunk.\n",
      "\n",
      "Talking to your vector database\n",
      "Following through this pipeline, you can successfully capture all the multimodal information present in the PDF. Here’s how the RAG pipeline works when a user asks a question.\n",
      "\n",
      "When a user prompts the system with a question, a simple RAG pipeline converts the question into an embedding and performs a semantic search to retrieve some relevant chunks of information. Considering that retrieved chunks also come from images, take a few additional steps before sending all the chunks to the LLM for generating the final response.\n",
      "\n",
      "Figure 5 shows a reference flow of how to tackle a user query to answer using information retrieved as chunks from both images and text. \n",
      "\n",
      "Diagram shows a workflow from a Slack interface, user query, NeMo Guardrails, a vector store and embedding model, breaking into similar chunks, separating into images, text from a chart/plot, and plain text, and in to an LLM for the final response.\n",
      "Figure 5. RAG inference workflow with customized MLLMs retrieving Information from multimodal data\n",
      "Here’s an example question prompting a multimodal RAG-enabled bot that has access to the PDF of interest, “What is the difference in performance between NVIDIA A100 and NVIDIA H100(v2.1) with 3D U-Net?”\n",
      "\n",
      "The pipeline was successful in retrieving the relevant graphical image, and interpreting it with an accurate mention of NVIDIA H100 (v2.1) having a 80% higher relative performance per accelerator than NVIDIA A100 on the 3D U-Net benchmark.\n",
      "\n",
      "The example shows the RAG pipeline’s capability to answer a question referring to both text and images from an NVIDIA Developer Blog post.\n",
      "Figure 6. Example question with answer extracted from a bar chart from the NVIDIA Developer Blog\n",
      "Here are some key steps involved in handling the question after performing a search and retrieving the top five relevant chunks: \n",
      "\n",
      "If the chunk was extracted from an image, an MLLM takes the image along with the user question as input to generate an answer. This is nothing but a VQA task. The generated answer is then used as the final context for an LLM to respond.\n",
      "If the chunk is extracted from a chart or plot, recall the linearized table stored as metadata and append the text as context to the LLM.\n",
      "Finally, the chunks coming from plain text are used as is.\n",
      "All these chunks, along with the user question, are now ready for the LLM to generate a final answer. From the sources listed in Figure 6, the bot referred to the chart that shows the relative performance on different benchmarks to generate an accurate final response. \n",
      "\n",
      "Extending the RAG pipeline\n",
      "This post touches on scenarios where simple text-based questions are answered using data spread across multiple modalities. To further the development of multimodal RAG technology and extend its capabilities, we recommend the following areas of research.\n",
      "\n",
      "Addressing user questions that include different modalities\n"
     ]
    }
   ],
   "source": [
    "print(document[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d5ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f418fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc7bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks=text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbcfeda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'train_data.txt'}, page_content='How do you manage information across modalities?'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Another crucial aspect is representing information across different modalities. For instance, if you are working with a document, you must make sure that the semantic representation of a chart aligns'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='of a chart aligns with the semantic representation of the text discussing the same chart.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Approaches for multimodal retrieval\\nWith the key challenge understood, here are the specifics of building RAG pipelines to tackle these challenges.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='There are several main approaches to building multi-modal RAG pipelines:'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Embed all modalities into the same vector space\\nGround all modalities into one primary modality\\nHave separate stores for different modalities'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='To keep this discussion concise, we only discuss images and text input.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Embed all modalities into the same vector space'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='In the case of images and text, you can use a model like CLIP to encode both text and images in the same vector space. This makes it so that you can largely use the same text-only RAG infrastructure'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='RAG infrastructure and swap out the embedding model to accommodate another modality. For the generation pass, you then replace the large language model (LLM) with a multimodal LLM (MLLM) for all'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='LLM (MLLM) for all question and answering.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='This approach simplifies the pipeline, as the only change required in the generic retrieval pipeline is that of swapping the embedding model.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='The tradeoff in this situation is to have access to a model that can effectively embed different types of images and text and also capture all the intricacies like text in images and complex tables.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Ground all modalities into one primary modality\\nAnother option is to pick a primary modality based on the focus of the application and ground all other modalities in the primary modality.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='For example, say your application revolves mainly around text-based Q&A over PDFs. In this case, you process text normally but for images, you create text descriptions and metadata in the'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='and metadata in the preprocessing step. You also store the images for later use.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='In the inference pass, the retrieval then works primarily off of the text description and metadata for the images, and the answer is generated with a mix of LLMs and MLLMs, depending on the type of'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='on the type of image retrieved.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='The key benefit here is that the metadata generated from the information-rich image is extremely helpful in answering objective questions. This also works around the need for tuning a new model for'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='a new model for embedding images as well as building a re-ranker to rank results from across different modalities. The key disadvantages are preprocessing costs and losing some nuance from the image.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='from the image.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Have separate stores for different modalities'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Rank-rerank is another approach where you have separate stores for different modalities, query them all to retrieve top-N chunks, and then have a dedicated multimodal re-ranker provide the most'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='provide the most relevant chunks.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='This approach simplifies the modeling process, so that you don’t have to align one model to work with multiple modalities. However, it adds complexity in the form of a re-ranker to arrange the now'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='to arrange the now top-M*N chunks (N each from M modalities).'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Multimodal models for generation'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='LLMs are designed to understand, interpret, and generate text-based information. Trained on vast amounts of textual data, LLMs can perform a range of natural language processing tasks, such as text'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='tasks, such as text generation, summarization, question-answering, and more.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='MLLMs can perceive more than textual data. MLLMs can handle modalities like images, audio, and video, which is often how real-world data is composed. They combine these different data types to create'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='types to create a more comprehensive interpretation of the information, improving the accuracy and robustness of its predictions.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='These models can perform a wide range of tasks:'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Visual language understanding and generation\\nMultimodal dialogue\\nImage captioning \\nVisual question answering (VQA)'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='These are all tasks that a RAG system can benefit from while dealing with multiple modalities. Having a deeper understanding of how MLLMs work with images and text requires a look at how these models'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='at how these models are constructed.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='One of the popular subtypes of MLLMs is Pix2Struct, a pretrained image-to-text model that enables semantic understanding of the visual input with its novel pretraining strategy. These models, as the'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='models, as the name suggests, generate structured information extracted from the image. For instance, a Pix2Struct model can extract the key information from charts and express it in text.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='With that understood, here’s how you can build a RAG pipeline.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Building a pipeline for multimodal RAG'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='To showcase how you can tackle different modalities of data, we walk you through an application indexing multiple technical posts, such as Breaking MLPerf Training Records with NVIDIA H100 GPUs. This'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='H100 GPUs. This post contains complex images that are charts and graphs with rich text, tabular data, and of course, paragraphs.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Here are the models and tools that you need before you can begin handling the data and building a RAG pipeline:'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='MLLM: Used for image captioning and VQA.\\nLLM: General reasoning and question answering.\\nEmbedding model: Encoding data into vectors.\\nVector database: Store the encoded vectors for retrieval.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Interpreting multimodal data and creating a vector database'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='The first step for building a RAG application is to preprocess your data and store it as vectors in a vector store so that you can then retrieve relevant vectors based on a query.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='With images present in the data, here is a generic RAG preprocessing workflow to work through (Figure 2).'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Demonstration of RAG preprocessing workflow for handling data containing images, showcasing the sequence of steps from data ingestion to processing and analysis.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Figure 2. General RAG preprocessing workflow for data with images'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='The post in question contains several bar charts like Figure 3. To interpret these bar charts, use Google’s DePlot, a visual-language model capable of comprehending charts and plots when coupled with'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='when coupled with an LLM. The model is available on NGC.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='For more information about using the DePlot API in your RAG application, see Query Graphs with Optimized DePlot Model.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='A bar chart image to be analyzed with an MLLM\\nFigure 3. Example bar chart from the PDF, including the caption'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='This example focuses on charts and plots. Other documents may contain images that may require model customization to handle specialized images, such as medical imagery or schematic diagrams. This'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='diagrams. This depends on the use case but you have several options to tackle this variance in imagery: Either tune one MLLM to handle all types of imagery or to build an ensemble of models for'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='of models for different types of images.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='To keep the explanation simple, this is a simple ensemble case with two categories:'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Image with graphs to be processed using DePlot\\nOther images to be processed with an MLLM like KOSMOS2'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='In this post, we expand the preprocessing pipeline to dive deeper into handling each modality in the pipeline that leverages custom text splitters, customized MLLMs, and LLMs to create the VectorDB'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='create the VectorDB (Figure 4).'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Diagram shows extracting text, separating images and text, building image descriptions, linearizing tables as text, splitting text, and other processing steps.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Figure 4. RAG preprocessing workflow with customized MLLMs tackling different types of images\\nHere are some key steps in the preprocessing workflow:'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Separate images and text\\nClassify images using an MLLM based on the image types\\nEmbed text in PDFs\\nSeparate images and text'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='The goal is to ground images to text modality. Start by extracting and cleaning your data to separate images and text. You can then go about tackling these two modalities to eventually store them in'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='store them in the vector store.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Classify images using an MLLM based on the image types'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Image descriptions generated by an MLLM can be used to classify images into categories whether or not they are graphs. Based on the classification, use DePlot for images containing graphs to generate'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='graphs to generate a linearized tabular text. This text being semantically different from regular text poses a challenge for retrieving the relevant information when performing a search during'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='a search during inference.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='We recommend using summaries of the linearized text as chunks to store in the vector store with the outputs from customized MLLMs as metadata, which you can use during inference.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Embed text in PDFs'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='There is room for exploring various text-splitting techniques based on the data that you are working with to achieve the best RAG performance. For simplicity, store each paragraph as a chunk.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Talking to your vector database'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Following through this pipeline, you can successfully capture all the multimodal information present in the PDF. Here’s how the RAG pipeline works when a user asks a question.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='When a user prompts the system with a question, a simple RAG pipeline converts the question into an embedding and performs a semantic search to retrieve some relevant chunks of information.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='of information. Considering that retrieved chunks also come from images, take a few additional steps before sending all the chunks to the LLM for generating the final response.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Figure 5 shows a reference flow of how to tackle a user query to answer using information retrieved as chunks from both images and text.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Diagram shows a workflow from a Slack interface, user query, NeMo Guardrails, a vector store and embedding model, breaking into similar chunks, separating into images, text from a chart/plot, and'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='a chart/plot, and plain text, and in to an LLM for the final response.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Figure 5. RAG inference workflow with customized MLLMs retrieving Information from multimodal data'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Here’s an example question prompting a multimodal RAG-enabled bot that has access to the PDF of interest, “What is the difference in performance between NVIDIA A100 and NVIDIA H100(v2.1) with 3D'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='H100(v2.1) with 3D U-Net?”'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='The pipeline was successful in retrieving the relevant graphical image, and interpreting it with an accurate mention of NVIDIA H100 (v2.1) having a 80% higher relative performance per accelerator'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='per accelerator than NVIDIA A100 on the 3D U-Net benchmark.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='The example shows the RAG pipeline’s capability to answer a question referring to both text and images from an NVIDIA Developer Blog post.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Figure 6. Example question with answer extracted from a bar chart from the NVIDIA Developer Blog'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Here are some key steps involved in handling the question after performing a search and retrieving the top five relevant chunks:'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='If the chunk was extracted from an image, an MLLM takes the image along with the user question as input to generate an answer. This is nothing but a VQA task. The generated answer is then used as the'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='is then used as the final context for an LLM to respond.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='If the chunk is extracted from a chart or plot, recall the linearized table stored as metadata and append the text as context to the LLM.\\nFinally, the chunks coming from plain text are used as is.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='All these chunks, along with the user question, are now ready for the LLM to generate a final answer. From the sources listed in Figure 6, the bot referred to the chart that shows the relative'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='shows the relative performance on different benchmarks to generate an accurate final response.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Extending the RAG pipeline'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='This post touches on scenarios where simple text-based questions are answered using data spread across multiple modalities. To further the development of multimodal RAG technology and extend its'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='and extend its capabilities, we recommend the following areas of research.'),\n",
       " Document(metadata={'source': 'train_data.txt'}, page_content='Addressing user questions that include different modalities')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc83bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of a chart aligns with the semantic representation of the text discussing the same chart.\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75992e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8500d05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\hp\\miniconda3\\lib\\site-packages (0.3.34)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.77 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-openai) (0.3.78)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.104.2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-openai) (2.1.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.4.32)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.77->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.77->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.9.18)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.104.2->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29cf54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e7a07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "287d372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp313-cp313-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\miniconda3\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.12.0-cp313-cp313-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.1/18.2 MB 14.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.8/18.2 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 8.4/18.2 MB 14.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 11.3/18.2 MB 14.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 13.9/18.2 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 15.7/18.2 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 13.1 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44c91a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25ec291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de1c6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1db12065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94a75276",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"jclemens24/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6d8c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3b6bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46b093e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model=ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20834e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain=(\n",
    "    {\"context\": retriever, \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    |llm_model\n",
    "    |output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a407fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To embed text in PDFs, you need to first separate images and text, classify images using an MLLM based on image types, and then embed the text into the PDFs. This process allows for capturing all intricacies like text in images and complex tables.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Embed text in PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089dedf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
